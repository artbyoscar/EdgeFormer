#!/usr/bin/env python3
"""
BERT Adapter for EdgeFormer Compression
Enables 8x compression for enterprise NLP applications including document processing,
sentiment analysis, question answering, and information extraction.
"""

import torch
import torch.nn as nn
import numpy as np
import json
import time
from pathlib import Path
import sys
from typing import Optional, Tuple, List

# Add the project root to the path so we can import modules
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

try:
    from src.compression.int4_quantization import INT4Quantizer
    from src.compression.utils import calculate_model_size
except ImportError:
    # Alternative import paths
    try:
        from compression.int4_quantization import INT4Quantizer
        from compression.utils import calculate_model_size
    except ImportError:
        print("‚ùå Cannot find compression modules. Please check your project structure.")
        sys.exit(1)

class BERTEmbeddings(nn.Module):
    """BERT embeddings module with word, position, and token type embeddings"""
    def __init__(self, vocab_size=30522, hidden_size=768, max_position_embeddings=512, 
                 type_vocab_size=2, layer_norm_eps=1e-12, dropout=0.1):
        super().__init__()
        self.word_embeddings = nn.Embedding(vocab_size, hidden_size)
        self.position_embeddings = nn.Embedding(max_position_embeddings, hidden_size)
        self.token_type_embeddings = nn.Embedding(type_vocab_size, hidden_size)
        
        self.LayerNorm = nn.LayerNorm(hidden_size, eps=layer_norm_eps)
        self.dropout = nn.Dropout(dropout)
        
        # Register buffer for position_ids
        self.register_buffer("position_ids", torch.arange(max_position_embeddings).expand((1, -1)))
        
    def forward(self, input_ids, token_type_ids=None, position_ids=None):
        seq_length = input_ids.size(1)
        
        if position_ids is None:
            position_ids = self.position_ids[:, :seq_length]
        
        if token_type_ids is None:
            token_type_ids = torch.zeros_like(input_ids)
        
        # Get embeddings
        word_embeddings = self.word_embeddings(input_ids)
        position_embeddings = self.position_embeddings(position_ids)
        token_type_embeddings = self.token_type_embeddings(token_type_ids)
        
        # Combine embeddings
        embeddings = word_embeddings + position_embeddings + token_type_embeddings
        embeddings = self.LayerNorm(embeddings)
        embeddings = self.dropout(embeddings)
        
        return embeddings

class BERTSelfAttention(nn.Module):
    """BERT self-attention mechanism"""
    def __init__(self, hidden_size=768, num_attention_heads=12, dropout=0.1):
        super().__init__()
        if hidden_size % num_attention_heads != 0:
            raise ValueError(f"hidden_size {hidden_size} must be divisible by num_attention_heads {num_attention_heads}")
        
        self.num_attention_heads = num_attention_heads
        self.attention_head_size = hidden_size // num_attention_heads
        self.all_head_size = self.num_attention_heads * self.attention_head_size
        
        self.query = nn.Linear(hidden_size, self.all_head_size)
        self.key = nn.Linear(hidden_size, self.all_head_size)
        self.value = nn.Linear(hidden_size, self.all_head_size)
        
        self.dropout = nn.Dropout(dropout)
        
    def transpose_for_scores(self, x):
        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
        x = x.view(*new_x_shape)
        return x.permute(0, 2, 1, 3)
    
    def forward(self, hidden_states, attention_mask=None):
        # Generate Q, K, V
        query_layer = self.transpose_for_scores(self.query(hidden_states))
        key_layer = self.transpose_for_scores(self.key(hidden_states))
        value_layer = self.transpose_for_scores(self.value(hidden_states))
        
        # Compute attention scores
        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
        attention_scores = attention_scores / np.sqrt(self.attention_head_size)
        
        # Apply attention mask if provided
        if attention_mask is not None:
            attention_scores = attention_scores + attention_mask
        
        # Normalize and apply dropout
        attention_probs = torch.softmax(attention_scores, dim=-1)
        attention_probs = self.dropout(attention_probs)
        
        # Apply attention to values
        context_layer = torch.matmul(attention_probs, value_layer)
        
        # Reshape back
        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
        context_layer = context_layer.view(*new_context_layer_shape)
        
        return context_layer

class BERTSelfOutput(nn.Module):
    """BERT self-attention output layer"""
    def __init__(self, hidden_size=768, dropout=0.1, layer_norm_eps=1e-12):
        super().__init__()
        self.dense = nn.Linear(hidden_size, hidden_size)
        self.LayerNorm = nn.LayerNorm(hidden_size, eps=layer_norm_eps)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, hidden_states, input_tensor):
        hidden_states = self.dense(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.LayerNorm(hidden_states + input_tensor)
        return hidden_states

class BERTAttention(nn.Module):
    """Complete BERT attention block"""
    def __init__(self, hidden_size=768, num_attention_heads=12, dropout=0.1, layer_norm_eps=1e-12):
        super().__init__()
        self.self = BERTSelfAttention(hidden_size, num_attention_heads, dropout)
        self.output = BERTSelfOutput(hidden_size, dropout, layer_norm_eps)
        
    def forward(self, hidden_states, attention_mask=None):
        self_outputs = self.self(hidden_states, attention_mask)
        attention_output = self.output(self_outputs, hidden_states)
        return attention_output

class BERTIntermediate(nn.Module):
    """BERT intermediate (feed-forward) layer"""
    def __init__(self, hidden_size=768, intermediate_size=3072):
        super().__init__()
        self.dense = nn.Linear(hidden_size, intermediate_size)
        self.intermediate_act_fn = nn.GELU()
        
    def forward(self, hidden_states):
        hidden_states = self.dense(hidden_states)
        hidden_states = self.intermediate_act_fn(hidden_states)
        return hidden_states

class BERTOutput(nn.Module):
    """BERT output layer"""
    def __init__(self, intermediate_size=3072, hidden_size=768, dropout=0.1, layer_norm_eps=1e-12):
        super().__init__()
        self.dense = nn.Linear(intermediate_size, hidden_size)
        self.LayerNorm = nn.LayerNorm(hidden_size, eps=layer_norm_eps)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, hidden_states, input_tensor):
        hidden_states = self.dense(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.LayerNorm(hidden_states + input_tensor)
        return hidden_states

class BERTLayer(nn.Module):
    """Single BERT transformer layer"""
    def __init__(self, hidden_size=768, num_attention_heads=12, intermediate_size=3072, 
                 dropout=0.1, layer_norm_eps=1e-12):
        super().__init__()
        self.attention = BERTAttention(hidden_size, num_attention_heads, dropout, layer_norm_eps)
        self.intermediate = BERTIntermediate(hidden_size, intermediate_size)
        self.output = BERTOutput(intermediate_size, hidden_size, dropout, layer_norm_eps)
        
    def forward(self, hidden_states, attention_mask=None):
        attention_output = self.attention(hidden_states, attention_mask)
        intermediate_output = self.intermediate(attention_output)
        layer_output = self.output(intermediate_output, attention_output)
        return layer_output

class BERTEncoder(nn.Module):
    """BERT encoder with multiple transformer layers"""
    def __init__(self, num_hidden_layers=12, hidden_size=768, num_attention_heads=12, 
                 intermediate_size=3072, dropout=0.1, layer_norm_eps=1e-12):
        super().__init__()
        self.layer = nn.ModuleList([
            BERTLayer(hidden_size, num_attention_heads, intermediate_size, dropout, layer_norm_eps)
            for _ in range(num_hidden_layers)
        ])
        
    def forward(self, hidden_states, attention_mask=None):
        for layer_module in self.layer:
            hidden_states = layer_module(hidden_states, attention_mask)
        return hidden_states

class BERTPooler(nn.Module):
    """BERT pooler for sequence classification"""
    def __init__(self, hidden_size=768):
        super().__init__()
        self.dense = nn.Linear(hidden_size, hidden_size)
        self.activation = nn.Tanh()
        
    def forward(self, hidden_states):
        # Pool the first token (CLS token)
        first_token_tensor = hidden_states[:, 0]
        pooled_output = self.dense(first_token_tensor)
        pooled_output = self.activation(pooled_output)
        return pooled_output

class BERTModel(nn.Module):
    """Complete BERT model for various NLP tasks"""
    def __init__(self, vocab_size=30522, hidden_size=768, num_hidden_layers=12, 
                 num_attention_heads=12, intermediate_size=3072, max_position_embeddings=512,
                 type_vocab_size=2, dropout=0.1, layer_norm_eps=1e-12):
        super().__init__()
        self.embeddings = BERTEmbeddings(vocab_size, hidden_size, max_position_embeddings, 
                                       type_vocab_size, layer_norm_eps, dropout)
        self.encoder = BERTEncoder(num_hidden_layers, hidden_size, num_attention_heads, 
                                 intermediate_size, dropout, layer_norm_eps)
        self.pooler = BERTPooler(hidden_size)
        
        # Initialize weights
        self.apply(self._init_weights)
        
    def _init_weights(self, module):
        """Initialize weights"""
        if isinstance(module, nn.Linear):
            nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            nn.init.normal_(module.weight, mean=0.0, std=0.02)
        elif isinstance(module, nn.LayerNorm):
            nn.init.zeros_(module.bias)
            nn.init.ones_(module.weight)
    
    def forward(self, input_ids, attention_mask=None, token_type_ids=None):
        # Create attention mask if not provided
        if attention_mask is None:
            attention_mask = torch.ones_like(input_ids)
        
        # Expand attention mask
        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
        extended_attention_mask = extended_attention_mask.to(dtype=torch.float32)
        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0
        
        # Forward pass
        embedding_output = self.embeddings(input_ids, token_type_ids)
        encoder_output = self.encoder(embedding_output, extended_attention_mask)
        pooled_output = self.pooler(encoder_output)
        
        return encoder_output, pooled_output

class BERTForSequenceClassification(nn.Module):
    """BERT model for sequence classification tasks"""
    def __init__(self, num_labels=2, **bert_config):
        super().__init__()
        self.bert = BERTModel(**bert_config)
        self.dropout = nn.Dropout(bert_config.get('dropout', 0.1))
        self.classifier = nn.Linear(bert_config.get('hidden_size', 768), num_labels)
        
    def forward(self, input_ids, attention_mask=None, token_type_ids=None):
        _, pooled_output = self.bert(input_ids, attention_mask, token_type_ids)
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        return logits

class BERTAdapter:
    """Adapter for compressing BERT models with EdgeFormer"""
    
    def __init__(self):
        self.quantizer = INT4Quantizer()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"BERT Adapter initialized on device: {self.device}")
        
    def create_bert_models(self):
        """Create BERT models of different sizes for testing"""
        models = {
            "BERT-Tiny": {
                "config": {
                    "vocab_size": 30522,
                    "hidden_size": 128,
                    "num_hidden_layers": 2,
                    "num_attention_heads": 2,
                    "intermediate_size": 512,
                    "max_position_embeddings": 512
                }
            },
            "BERT-Mini": {
                "config": {
                    "vocab_size": 30522,
                    "hidden_size": 256,
                    "num_hidden_layers": 4,
                    "num_attention_heads": 4,
                    "intermediate_size": 1024,
                    "max_position_embeddings": 512
                }
            },
            "BERT-Small": {
                "config": {
                    "vocab_size": 30522,
                    "hidden_size": 512,
                    "num_hidden_layers": 6,
                    "num_attention_heads": 8,
                    "intermediate_size": 2048,
                    "max_position_embeddings": 512
                }
            },
            "BERT-Base": {
                "config": {
                    "vocab_size": 30522,
                    "hidden_size": 768,
                    "num_hidden_layers": 12,
                    "num_attention_heads": 12,
                    "intermediate_size": 3072,
                    "max_position_embeddings": 512
                }
            }
        }
        
        for name, info in models.items():
            config = info["config"]
            # Create BERT for sequence classification with 2 labels (binary classification)
            model = BERTForSequenceClassification(num_labels=2, **config)
            models[name]["model"] = model
            
        return models
    
    def compress_bert_model(self, model, model_name):
        """Compress a BERT model using INT4 quantization"""
        print(f"\n{'='*60}")
        print(f"COMPRESSING {model_name}")
        print(f"{'='*60}")
        
        # Move model to device
        model = model.to(self.device)
        model.eval()
        
        # Calculate original size
        original_size = calculate_model_size(model)
        
        compression_results = {
            "model_config": {
                "name": model_name,
                "vocab_size": model.bert.embeddings.word_embeddings.num_embeddings,
                "hidden_size": model.bert.embeddings.word_embeddings.embedding_dim,
                "num_layers": len(model.bert.encoder.layer),
                "num_heads": model.bert.encoder.layer[0].attention.self.num_attention_heads if model.bert.encoder.layer else 0,
                "intermediate_size": model.bert.encoder.layer[0].intermediate.dense.out_features if model.bert.encoder.layer else 0,
                "max_position_embeddings": model.bert.embeddings.position_embeddings.num_embeddings
            },
            "compression": {
                "total_layers_tested": 0,
                "successful_layers": 0,
                "success_rate": 0.0,
                "overall_compression": 0.0,
                "avg_compression": 0.0,
                "avg_accuracy_loss": 0.0,
                "original_size_mb": original_size,
                "compressed_size_mb": 0.0,
                "detailed_results": []
            }
        }
        
        total_layers = 0
        successful_layers = 0
        total_compression_sum = 0.0
        total_accuracy_loss = 0.0
        compressed_size = 0.0
        
        # Compress each layer
        for name, param in model.named_parameters():
            if param.requires_grad and len(param.shape) >= 2:
                total_layers += 1
                
                print(f"\nLayer: {name}")
                print(f"Shape: {list(param.shape)}")
                
                try:
                    # Quantize the parameter
                    quantized_param, scale, zero_point = self.quantizer.quantize_tensor(param)
                    
                    # Calculate compression metrics
                    original_param_size = param.numel() * 4  # float32 = 4 bytes
                    compressed_param_size = quantized_param.numel() * 0.5  # int4 = 0.5 bytes
                    layer_compression = original_param_size / compressed_param_size
                    
                    # Calculate accuracy loss (reconstruction error)
                    dequantized = self.quantizer.dequantize_tensor(quantized_param, scale, zero_point)
                    accuracy_loss = torch.mean(torch.abs(param - dequantized)).item() * 100
                    
                    successful_layers += 1
                    total_compression_sum += layer_compression
                    total_accuracy_loss += accuracy_loss
                    compressed_size += compressed_param_size / (1024 * 1024)  # Convert to MB
                    
                    # Store detailed results
                    layer_result = {
                        "layer": name,
                        "shape": list(param.shape),
                        "compression": layer_compression,
                        "accuracy_loss": accuracy_loss,
                        "original_size_mb": original_param_size / (1024 * 1024),
                        "compressed_size_mb": compressed_param_size / (1024 * 1024)
                    }
                    compression_results["compression"]["detailed_results"].append(layer_result)
                    
                    print(f"‚úÖ Compression: {layer_compression:.1f}x")
                    print(f"‚úÖ Accuracy Loss: {accuracy_loss:.3f}%")
                    
                except Exception as e:
                    print(f"‚ùå Failed: {str(e)}")
                    continue
        
        # Calculate overall metrics
        if successful_layers > 0:
            compression_results["compression"].update({
                "total_layers_tested": total_layers,
                "successful_layers": successful_layers,
                "success_rate": (successful_layers / total_layers) * 100,
                "overall_compression": total_compression_sum / successful_layers,
                "avg_compression": total_compression_sum / successful_layers,
                "avg_accuracy_loss": total_accuracy_loss / successful_layers,
                "compressed_size_mb": compressed_size
            })
        
        return compression_results
    
    def test_bert_inference(self, model, model_name):
        """Test inference with the BERT model"""
        print(f"\n{'='*40}")
        print(f"TESTING {model_name} INFERENCE")
        print(f"{'='*40}")
        
        model.eval()
        
        # Create sample input (sequence length 128, batch size 2)
        batch_size = 2
        seq_length = 128
        vocab_size = model.bert.embeddings.word_embeddings.num_embeddings
        
        # Create random token IDs (avoiding padding token 0)
        sample_input_ids = torch.randint(1, min(1000, vocab_size), (batch_size, seq_length)).to(self.device)
        sample_attention_mask = torch.ones(batch_size, seq_length).to(self.device)
        
        inference_results = {
            "inference_time_ms": 0.0,
            "sequences_per_second": 0.0,
            "output_shape": [],
            "output_valid": False
        }
        
        try:
            with torch.no_grad():
                start_time = time.time()
                outputs = model(sample_input_ids, attention_mask=sample_attention_mask)
                end_time = time.time()
                
                inference_time = (end_time - start_time) * 1000  # Convert to ms
                sequences_per_second = batch_size / (inference_time / 1000)
                
                inference_results.update({
                    "inference_time_ms": inference_time,
                    "sequences_per_second": sequences_per_second,
                    "output_shape": list(outputs.shape),
                    "output_valid": True
                })
                
                print(f"‚úÖ Inference Time: {inference_time:.2f} ms")
                print(f"‚úÖ Sequences/Second: {sequences_per_second:.1f}")
                print(f"‚úÖ Output Shape: {list(outputs.shape)}")
                print(f"‚úÖ Output Range: [{outputs.min().item():.3f}, {outputs.max().item():.3f}]")
                
        except Exception as e:
            print(f"‚ùå Inference failed: {str(e)}")
            inference_results["output_valid"] = False
            
        return inference_results
    
    def test_text_classification(self, model, model_name):
        """Test text classification with the BERT model"""
        print(f"\n{'='*40}")
        print(f"TESTING {model_name} CLASSIFICATION")
        print(f"{'='*40}")
        
        model.eval()
        
        # Create sample sequences for classification
        batch_size = 4
        seq_length = 64
        vocab_size = model.bert.embeddings.word_embeddings.num_embeddings
        
        sample_input_ids = torch.randint(1, min(1000, vocab_size), (batch_size, seq_length)).to(self.device)
        sample_attention_mask = torch.ones(batch_size, seq_length).to(self.device)
        
        classification_results = {
            "classification_successful": False,
            "predictions": [],
            "confidence_scores": []
        }
        
        try:
            with torch.no_grad():
                logits = model(sample_input_ids, attention_mask=sample_attention_mask)
                probabilities = torch.softmax(logits, dim=1)
                
                # Get predictions
                predictions = torch.argmax(logits, dim=1)
                confidence_scores = torch.max(probabilities, dim=1)[0]
                
                classification_results.update({
                    "classification_successful": True,
                    "predictions": predictions.cpu().tolist(),
                    "confidence_scores": confidence_scores.cpu().tolist()
                })
                
                print(f"‚úÖ Classification successful")
                print(f"‚úÖ Predictions: {predictions.cpu().tolist()}")
                print(f"‚úÖ Confidence Scores: {[f'{score:.3f}' for score in confidence_scores.cpu().tolist()]}")
                
        except Exception as e:
            print(f"‚ùå Classification failed: {str(e)}")
            
        return classification_results
    
    def run_comprehensive_test(self):
        """Run comprehensive BERT compression testing"""
        print("üî¨ EdgeFormer BERT Compression Test")
        print("=" * 70)
        
        # Create BERT models
        models = self.create_bert_models()
        all_results = []
        
        for model_name, model_info in models.items():
            model = model_info["model"]
            
            # Compress the model
            compression_results = self.compress_bert_model(model, model_name)
            
            # Test inference
            inference_results = self.test_bert_inference(model, model_name)
            compression_results["inference"] = inference_results
            
            # Test classification
            classification_results = self.test_text_classification(model, model_name)
            compression_results["classification"] = classification_results
            
            all_results.append(compression_results)
            
            # Print summary
            print(f"\nüéØ {model_name} SUMMARY:")
            print(f"   Compression: {compression_results['compression']['overall_compression']:.1f}x")
            print(f"   Success Rate: {compression_results['compression']['success_rate']:.1f}%")
            print(f"   Accuracy Loss: {compression_results['compression']['avg_accuracy_loss']:.3f}%")
            print(f"   Size Reduction: {compression_results['compression']['original_size_mb']:.1f}MB ‚Üí {compression_results['compression']['compressed_size_mb']:.1f}MB")
        
        return all_results
    
    def save_results(self, results, filename="bert_compression_results.json"):
        """Save compression results to JSON file"""
        output_dir = Path("results")
        output_dir.mkdir(exist_ok=True)
        
        output_path = output_dir / filename
        with open(output_path, 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"\nüíæ Results saved to: {output_path}")
        return output_path

def main():
    """Main function to run BERT compression testing"""
    print("üöÄ Starting BERT Compression Test")
    print("=" * 70)
    
    # Create adapter
    adapter = BERTAdapter()
    
    # Run comprehensive testing
    results = adapter.run_comprehensive_test()
    
    # Save results
    adapter.save_results(results)
    
    # Print final summary
    print("\n" + "=" * 70)
    print("üéâ BERT COMPRESSION COMPLETE")
    print("=" * 70)
    
    for result in results:
        model_name = result["model_config"]["name"]
        compression = result["compression"]["overall_compression"]
        success_rate = result["compression"]["success_rate"]
        accuracy_loss = result["compression"]["avg_accuracy_loss"]
        
        print(f"üìä {model_name}:")
        print(f"   üîÑ Compression: {compression:.1f}x")
        print(f"   ‚úÖ Success Rate: {success_rate:.1f}%")
        print(f"   üìà Accuracy Loss: {accuracy_loss:.3f}%")
        print()
    
    print("üéØ BERT compression validation complete!")
    print("üíº Ready for enterprise NLP, document processing, and information extraction!")

if __name__ == "__main__":
    main()